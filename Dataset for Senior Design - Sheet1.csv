Selection Sort has a Best Case: 𝛀(n^2)
Selection Sort has Average Case: θ(n^2)
Selection Sort has a Worst Case: O(n^2)
Selection Sort has Space Complexity: O(1)
Bubble Sort has a Best Case: 𝛀(n)
Bubble Sort has Average Case: θ(n^2)
Bubble Sort has a Worst Case: O(n^2)
Bubble Sort has Space Complexity: O(1)
Insertion Sort has a Best Case: 𝛀(n)
Insertion Sort has Average Case: θ(n^2)
Insertion Sort has a Worst Case: O(n^2)
Insertion Sort has Space Complexity: O(n)
Heap Sort has a Best Case: 𝛀(n*lg(n))
Heap Sort has Average Case: θ(n*lg(n))
Heap Sort has a Worst Case: O(n*lg(n))
Heap Sort has Space Complexity: O(1)
Quick Sort has a Best Case: 𝛀(n*lg(n))
Quick Sort has Average Case: θ(n*lg(n))
Quick Sort has a Worst Case: O(n^2)
Quick Sort has Space Complexity: O(n)
Merge Sort has a Best Case: 𝛀(n*lg(n))
Merge Sort has Average Case: θ(n*lg(n))
Merge Sort has a Worst Case: O(n*lg(n))
Merge Sort has Space Complexity: O(n)
Bucket Sort has a Best Case: 𝛀(n+k)
Bucket Sort has Average Case: θ(n+k)
Bucket Sort has a Worst Case: O(n^2)
Bucket Sort has Space Complexity: O(n)
Radix Sort has a Best Case: 𝛀(n*k)
Radix Sort has Average Case: θ(n*k)
Radix Sort has a Worst Case: O(n*k)
Radix Sort has Space Complexity: O(n+k)
Count Sort has a Best Case: 𝛀(n+k)
Count Sort has Average Case: θ(n+k)
Count Sort has a Worst Case: O(n+k)
Count Sort has Space Complexity: O(k)
Shell Sort has a Best Case: 𝛀(n*lg(n))
Shell Sort has Average Case: θ(n*lg(n))
Shell Sort has a Worst Case: O(n^2)
Shell Sort has Space Complexity: O(1)
Tim Sort has a Best Case: 𝛀(n)
Tim Sort has Average Case: θ(n*lg(n))
Tim Sort has a Worst Case: O(n*lg(n))
Tim Sort has Space Complexity: O(n)
Tree Sort has a Best Case: 𝛀(n*lg(n))
Tree Sort has Average Case: θ(n*lg(n))
Tree Sort has a Worst Case: O(n*lg(n))
Tree Sort has Space Complexity: O(n)
Cube Sort has a Best Case: 𝛀(n)
Cube Sort has Average Case: θ(n*lg(n))
Cube Sort has a Worst Case: O(n^2)
Cube Sort has Space Complexity: O(n)
Bogo Sort has a Best Case: 𝛀(n)
"Bogo Sort has Average Case: θ((n-1)*n!)"
Bogo Sort has a Worst Case: O(∞)
Bogo Sort has Space Complexity: O(1)
Quick Sort has a Best Case: 𝛀(n*lg(n))
"Quick Sort has Average Case: θ(n^2)"
Quick Sort has a Worst Case: O(n^2)
Quick Sort has Space Complexity: O(1)
Quantum Bogo Sort has a Best Case: 𝛀(n)
Quantum Bogo Sort has Average Case: θ(∞)
Quantum Bogo Sort has a Worst Case: O(∞)
Quantum Bogo Sort has Space Complexity: O(1)
Linear Search has a Best Case: 𝛀(1)
Linear Search has Average Case: θ(n/2) or just θ(n)
Linear Search has a Worst Case: O(n)
Linear Search has Space Complexity: O(1)
Binary Search has a Best Case: 𝛀(1)
Binary Search has Average Case: θ(lg(n))
Binary Search has a Worst Case: O(lg(n))
Binary Search has Space Complexity: O(1)
Jump Search has a Best Case: 𝛀(1)
Jump Search has Average Case: θ(n/2)
Jump Search has a Worst Case: O(n)
Jump Search has Space Complexity: O(1)
Interpolation Search has a Best Case: 𝛀(1). This occurs when the middle (our approximation) is the desired key.
Interpolation Search has Average Case: θ(lg(n)) It achieves this when the elements are uniformly distributed.
Interpolation Search has a Worst Case: O(n) This occurs when the numerical values of hte keys increase exponentially.
Interpolation Search has Space Complexity: O(1)
Exponential Search has a Best Case: 𝛀(1)
Exponential Search has Average Case: θ(lg(n))
Exponential Search has a Worst Case: O(lg(n))
Exponential Search has Space Complexity: O(lg(n))
Sublist Search has a Best Case: 𝛀(n)
Sublist Search has Average Case: θ(n)
Sublist Search has a Worst Case: O(n*m) where N is the length of a list and M is the length of sublist
Sublist Search has Space Complexity: O(1)
Fibonacci Search has a Best Case: 𝛀(1)
Fibonacci Search has Average Case: θ(n/2)
Fibonacci Search has a Worst Case: O(n)
Fibonacci Search has Space Complexity: O(1)
Definition of Time Complexity: the computational complexity to calculate the amount of resources (computational power as well as time) to run an algorithm.
"Time complexity takes into account how many actions are taken, but is generally focused on the number of accesses and variables uses."
"For example, bubble sort has to iterate an entire array of size ‘n’ to confirm that only the last variable is in place. There are still ‘n-1’ positions to check, and therefore has to run ‘n’ times over an input array of size ‘n’. Hence, it’s time complexity is O(n^2)."
Definition of Space Complexity: the computation complexity to calculate the amount of space or storage required to run an algorithm.
Space complexity takes into account how many variables or structs are required to keep track of the algorithm to be able to carry it out.
"For example, bubble sort only requires enough space to hold a temporary variable in order to sort and thus only has a space complexity of O(1) or constant."
"Big-Oh or Big-O is the measure of the “worst case” computation and follows the format: O(variable), for example bubblesort runs in n^2 worst time, so O(n^2)."
"Many algorithms have a very good “best case” scenario, but as in most algorithms that perform exceedingly well in certain scenarios, the “worst case” is often times more prevalent and therefore what most look out for."
"For example, in most cases, bubble sort may find a small number on the very end, as it will only ever shift 1 position over (if you are sorting ascending, then it starts on the far right side, and shifts to the left only a single position every iteration)"
"Omega or 𝛀 is the measure of the “best case” or more formally, the “lower bound” computation and follows the format 𝛀(variable)."
"For example bubblesort runs in 𝛀(n) as in the best case, it only has to run through the array a single time."
In general we try to achieve Omega consistently.
Theta or θ is the measure of the “average case” or average time it takes to compute an algorithm and follows the format θ(variable).
"Foro example, bubblesort runs in θ(n^2) as it has a tendency to follow the worst case and not the best case."
"In general we about having a tight bound and ensuring the average case is as low as possible, especially when the average case is close or identical to the best case."
"Definition of Tightly bound: If the 𝛀() and O() (more formally upper and lower bound) are the same or roughly the same, such that the average case or the θ(algorithm) is both the best and worst case, then it is considered tightly bound."
"For example, MergeSort’s worst case and best case are both O(n*lg(n)) and therefore so too is the average case. Therefore, MergeSort is considered a tightly bound sort."
Definition of upperbound: This is the maximal amount of time it will take to compute an algorithm in run time.
"Essentially, upperbound is just the “worst case” scenario and is interchangeably used as such, or even denoted with Big-O: O(n) for example."
Definition of lowerbound: This is the minimal amount of time it will take to compute an algorithm in run time.
"Essentially, lowerbound is just the “best case” scenario and is interchangeably used as such, or even denoted with Omega or 𝛀(n) for example."
"Definition of lg(n): It is important to note that in computer science due to the convenience and how binary works, we typically describe log as lg. This denotation specifies that instead of using logarithm as base 10, we instead use a base 2."
"It is key to note that lg and log are NOT the same thing, however in many textbooks and depending on professors and classes, they are frequently used interchangeably unless otherwise specified, but in general the intent is in reference to base 2."
"If you ever are uncertain or have questions, about which to use, please speak to the teacher or TA."
Definition of Naive Algorithm: A simple algorithm that may or may not find the optimal solution and is often not the quickest.
"Definition of Greedy Algorithm: An algorithm that makes the best possible choice in the short term, that quickly finds an answer although it may not be optimal."
"Definition of Optimal answer: A path or solution that can be calculated and proven to be (one of, if not) the best possible answer or path."
"Sequential Search: In this, the list or array is traversed sequentially and every element is checked."
Interval Search: These algorithms are specifically designed for searching in sorted data-structures.
These type of searching algorithms are much more efficient than Linear Search as they repeatedly target the center of the search structure and divide the search space in half
"Definition of Divide and Conquer: An approach in algorithms where you break up problems into smaller more manageable chunks that are then solvable, and then these smaller parts are used to solve the algorithm as a whole."